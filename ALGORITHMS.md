# üìö Algorithm Documentation

## Overview of Implemented Algorithms

### 1. Preconditioned Conjugate Gradient (PCG) Solver
**Purpose**: Efficiently solve large sparse linear systems Ax = b

**Algorithm**: 
- Uses incomplete Cholesky decomposition as preconditioner
- Achieves O(n‚àöŒ∫) convergence rate where Œ∫ is condition number
- Particularly effective for positive definite systems

**Mathematical Foundation**:
```
minimize œÜ(x) = (1/2)x^T A x - b^T x
```

**Key Features**:
- Memory efficient: O(n) storage
- Robust for ill-conditioned systems
- Adaptive convergence criteria

---

### 2. Newton Descent Method
**Purpose**: Find local minima of twice-differentiable functions

**Algorithm**:
- Uses second-order Taylor approximation
- Requires Hessian matrix computation
- Q-quadratic convergence near solution

**Mathematical Foundation**:
```
x_{k+1} = x_k - H_k^{-1} ‚àáf(x_k)
where H_k is the Hessian matrix
```

**Convergence Properties**:
- Local q-quadratic convergence
- Requires positive definite Hessian
- Fast convergence in neighborhood of solution

---

### 3. BFGS Quasi-Newton Method
**Purpose**: Approximate Newton method without computing Hessian

**Algorithm**:
- Builds Hessian approximation using gradient information
- Uses rank-2 updates (BFGS formula)
- Maintains positive definiteness

**Mathematical Foundation**:
```
B_{k+1} = B_k + (y_k y_k^T)/(y_k^T s_k) - (B_k s_k s_k^T B_k)/(s_k^T B_k s_k)
where s_k = x_{k+1} - x_k, y_k = ‚àáf(x_{k+1}) - ‚àáf(x_k)
```

**Advantages**:
- No Hessian computation required
- Superlinear convergence
- Memory efficient with L-BFGS variant

---

### 4. Wolfe-Powell Line Search
**Purpose**: Find suitable step sizes satisfying strong Wolfe conditions

**Conditions**:
1. **Sufficient Decrease**: f(x + Œ±p) ‚â§ f(x) + c‚ÇÅŒ±‚àáf(x)^T p
2. **Curvature Condition**: |‚àáf(x + Œ±p)^T p| ‚â§ c‚ÇÇ|‚àáf(x)^T p|

**Parameters**: 
- c‚ÇÅ ‚àà (0, 1/2): typically 10‚Åª‚Å¥
- c‚ÇÇ ‚àà (c‚ÇÅ, 1): typically 0.9

---

### 5. Projected Methods for Box Constraints
**Purpose**: Solve optimization problems with simple bounds

**Constraint Set**: x ‚àà [a, b] = {x : a·µ¢ ‚â§ x·µ¢ ‚â§ b·µ¢}

**Projection Operator**:
```
P(x)·µ¢ = max(a·µ¢, min(x·µ¢, b·µ¢))
```

**Key Algorithm**: Projected Inexact Newton-CG
- Combines Newton method with CG solver
- Handles active constraints through projection
- Maintains feasibility throughout iterations

---

### 6. Levenberg-Marquardt Algorithm
**Purpose**: Solve non-linear least squares problems

**Problem Formulation**:
```
minimize f(x) = (1/2) Œ£·µ¢ r·µ¢(x)¬≤
where r·µ¢(x) are residual functions
```

**Algorithm Update**:
```
(J^T J + ŒªI) Œ¥ = -J^T r
x_{k+1} = x_k + Œ¥
```

**Adaptive Damping**: Œª parameter balances between Gauss-Newton and gradient descent

---

### 7. Augmented Lagrangian Method
**Purpose**: Handle equality constraints and box constraints simultaneously

**Augmented Lagrangian**:
```
L_A(x, Œª, Œº) = f(x) + Œª^T h(x) + (Œº/2) ||h(x)||¬≤
```

**Algorithm**:
- Alternate between minimizing augmented Lagrangian
- Update multipliers: Œª ‚Üê Œª + Œºh(x)
- Increase penalty parameter Œº when needed

**Convergence**: Achieves superlinear convergence to KKT points
